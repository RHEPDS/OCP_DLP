Distance Learning
ARCHITECTURAL OVERVIEW
1
Alfred Bach
Principal Solution Architect

OpenShift
Container Platform

Part 1 
(16:15 - 17:00)
Part 2 
(17:05 - 18:00)
Part 3
Day 2 (16:05 - 17:00)
OpenShift Architecture

3
Functional overview

OPENSHIFT CONTAINER PLATFORM | Technical Value
4
Self-Service
Multi-language
Automation
Collaboration
Multi-tenant
Standards-based
Web-scale
Open Source
Enterprise Grade
Secure

Value of OpenShift
OPENSHIFT CONTAINER PLATFORM | Functional Overview
5
Red Hat Enterprise Linux | RHEL CoreOS
Kubernetes
Automated Operations
Cluster Services
Monitoring, Logging, Registry, Router, Telemetry
Developer Services
Dev Tools, CI/CD, Automated Builds, IDE
Application Services
Service Mesh, Serverless, Middleware/Runtimes, ISVs
CaaS
PaaS
Best IT Ops Experience
Best Developer Experience
FaaS

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
6
EXISTING AUTOMATION TOOLSETS
SCM
(GIT)
CI/CD
WORKER
MASTER
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
Router
Developers
Admins
WORKER
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
Router

CNCF Ecosystem Slide
7



10
OpenShift and Kubernetes core concepts

OpenShift Concepts
11
a container is the smallest compute unit
CONTAINER

OpenShift Concepts
12
containers are created from container images
BINARY
RUNTIME
CONTAINER
IMAGE

OpenShift Concepts
13
IMAGE REGISTRY
container images are stored in an image registry
CONTAINER
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE

OpenShift Concepts
14
an image repository contains all versions of an image in the image registry
IMAGE REGISTRY
frontend:latest
frontend:2.0
frontend:1.1
frontend:1.0
mongo:latest
mongo:3.7
mongo:3.6
mongo:3.4
myregistry/frontend
myregistry/mongo
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE
IMAGE

OpenShift Concepts
15
containers are wrapped in pods which are units of deployment and management





POD
CONTAINER
10.140.4.44





POD
CONTAINER
10.15.6.55
CONTAINER

OpenShift Concepts
16
ReplicationControllers & ReplicaSets ensure a specified number of pods are running at any given time
image name
replicas
labelscpumemory
storage
ReplicaSetReplicationController





POD
CONTAINER





POD
CONTAINER
...





POD
CONTAINER
1
2
N

OpenShift Concepts
17
Deployments and DeploymentConfigurations define how to roll out new versions of Pods
image name
replicas
labels
version
strategy
DeploymentDeploymentConfig





POD
CONTAINER





POD
CONTAINER
v1
v2

OpenShift Concepts
18
a daemonset ensures that all(or some) nodes run a copy of a pod
foo = bar
Node
image name
replicas
labelscpumemory
storage
DaemonSet
foo = bar
Node
foo = baz
Node





POD
CONTAINER





POD
CONTAINER
✓
✓

Dev
OpenShift Concepts
19
configmaps allow you to decouple configuration artifacts from image content
appconfig.conf


MYCONFIG=true
ConfigMap





POD
CONTAINER
Prod
appconfig.confMYCONFIG=false
ConfigMap





POD
CONTAINER

OpenShift Concepts
20
secrets provide a mechanism to hold sensitive information such as passwords
Dev
hash.pw


ZGV2Cg==
ConfigMap





POD
CONTAINER
Prod
hash.pwcHJvZAo=
ConfigMap





POD
CONTAINER
The etcd datastore can be encrypted for additional security
https://docs.openshift.com/container-platform/4.6/security/encrypting-etcd.html
Secure mechanism for holding sensitive data e.g.
Passwords and credentials
SSH Keys
Certificates
Secrets are made available as
Environment variables
Volume mounts
Interaction with external systems
Encrypted in transit and at rest
The etcd datastore can be encrypted. https://docs.openshift.com/container-platform/4.6/security/encrypting-etcd.html
When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:
Secrets
ConfigMaps
Routes
OAuth access tokens
OAuth authorize tokens
When you enable etcd encryption, OCP automatically creates the encryption keys. These keys are rotated on a weekly basis. You must have these keys in order to restore from an etcd backup.
Having an easy-to-use, secure-by-default secret distribution mechanism is exactly what developers and ops need to solve the secrets management problem once and for all
Because secret objects can be created independently of the pods that use them, there is less risk of the secret being exposed during the workflow of creating, viewing, and editing pods.
A secret is only sent to a node if a pod on that node requires it. It is not written to disk. It is stored in a temporary file-storage facilities (tmpfs). It is deleted once the pod that depends on it is deleted.
Secrets are stored as plaintext in etcd however the etcd storage can be encrypted to remove the security risk

21
OPENSHIFT & KUBERNETES CONCEPTS
services provide internal load-balancing and service discovery across pods





POD
SERVICE
“backend”
CONTAINER
10.110.1.11
role:backend





POD
CONTAINER
10.120.2.22
role:backend





POD
CONTAINER
10.130.3.33
role:backend





POD
CONTAINER
10.140.4.44
role:frontend
role:backend

22
OPENSHIFT & KUBERNETES CONCEPTS
apps can talk to each other via services





POD
SERVICE
“backend”
CONTAINER
10.110.1.11
role:backend





POD
CONTAINER
10.120.2.22
role:backend





POD
CONTAINER
10.130.3.33
role:backend





POD
CONTAINER
10.140.4.44
role:frontend
role:backend

OpenShift Concepts
23
routes make services accessible to clients outside the environment via real-world urls
> curl http://app-prod.mycompany.com




POD
SERVICE
“frontend”
CONTAINER
role:frontend




POD
CONTAINER
role:frontend




POD
CONTAINER
role:frontend
role:frontend
ROUTE
app-prod.mycompany.com
A route exposes a service at a host name, like www.example.com, so that external clients can reach it by name.
DNS resolution for a host name is handled separately from routing. Admin may have configured a DNS wildcard entry that will resolve to the node that is running the OpenShift Container Platform router
Pods running on OpenShift, don’t need to go through the routing layer and can interact with each other directly through the services.
After the router discovers the Pod endpoints via the service, it sends the Pod traffic directly to those endpoints and bypasses the service layer


OpenShift Concepts
24
Persistent Volume and Claims
My app is stateful.
2Gi
PersistentVolumeClaim
2Gi
PersistentVolume





POD
CONTAINER
OpenShift and Kubernetes can handle stateful applications that require storage, too. Users create claims for storage, and OpenShift connects those claims to real storage volumes. When applications are deployed, OpenShift automatically connects the real storage into the container as specified by the end user. And, as applications move around in the cluster, the storage automatically follows them. Many different storage types are supported, from raw to block to file, both read-write-once and read-write-many.

OpenShift Concepts
25
Liveness and Readiness
ready?
alive?
OpenShift includes two types of probes for evaluating the status of applications. A liveness probe examines whether an application is functioning properly and, if not, causes it to be restarted. A readiness probe examines whether an application is ready to receive traffic and, if not, causes it to be removed from the service endpoint list. Probes are powerful ways to increase the reliability and availability of applications in OpenShift.

OpenShift Concepts
26
projects isolate apps across environments, teams, groups and departments
PAYMENT DEV
PAYMENT PROD
CATALOG
INVENTORY
❌
❌
❌




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C




POD
C

Part 1 
(16:15 - 17:00)
Part 2 
(17:05 - 18:00)
Part 3
Day 2 (16:05 - 17:00)
OpenShift Architecture

28
OpenShift 4 Architecture

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
29
STORAGE
NETWORK
COMPUTE
your choice of infrastructure
OpenShift runs on a variety of infrastructure platforms, from bare metal to private cloud to public cloud. 

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
30
WORKER / NODE
STORAGE
NETWORK
COMPUTE
WORKER / NODE
workers run workloads
There are two types of hosts in an OpenShift environment. Workers are where user workload and various OpenShift components will land. Workers can be easily scaled and, on cloud providers, even scaled automatically based on cluster capacity.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
31
MASTER (NODE) / CONTROL PLANE
STORAGE
NETWORK
COMPUTE
masters are the control plane
The other type of host is the master. OpenShift uses 3 masters for high availability and cluster quorum. User workload does not run on the masters.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
32
everything runs in pods
CONTAINER
IMAGE





POD
CONTAINER
10.140.4.44
The pod is the deployable and scheduleable unit in an OpenShift environment. While a pod may have more than one container, all containers in the pod share a single network namespace, and the pod receives a single IP address on the software defined network inside the cluster.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
33
MASTER (NODE) / CONTROL PLANE
STORAGE
etcd
NETWORK
COMPUTE
state of everything
etcd is used to keep track of the state of everything in the cluster, from which users are logged in to where workload lives and more.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
34
MASTER (NODE) / CONTROL PLANE
STORAGE
Kubernetes
services
etcd
NETWORK
COMPUTE
core kubernetes components
Kubernetes
API server
Scheduler
Cluster Management
Introducing the Declarative Architecture of Kubernetes
The architecture of OpenShift is based on the declarative nature of Kubernetes. Most system administrators are used to imperative architectures, where you perform actions that indirectly change the state of the system, such as starting and stopping containers on a given server. In a declarative architecture, you change the state of the system and the system updates itself to comply with the new state. For example, with Kubernetes, you define a pod resource that specifies that a certain container should run under specific conditions. Then Kubernetes finds a server (a node) that can run that container under these specific conditions.
Declarative architectures allow for self-optimizing and self-healing systems that are easier to manage than imperative architectures.
Kubernetes defines the state of its cluster, including the set of deployed applications, as a set of resources stored in the etcd database. Kubernetes also runs controllers that monitor these resources and compares them to the current state of the cluster. These controllers take any action necessary to reconcile the state of the cluster with the state of the resources, for example by finding a node with sufficient CPU capacity to start a new container from a new pod resource.
Kubernetes provides a REST API to manage these resources. All actions that an OpenShift user takes, either using the command-line interface or the web console, are performed by invoking this REST API.

OpenShift is built on Kubernetes, and its core components are still there and directly accessible via Kubernetes’ APIs.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
35
MASTER (NODE) / CONTROL PLANE
OpenShift
services
STORAGE
Kubernetes
services
etcd
NETWORK
COMPUTE
core OpenShift components
OpenShift
API server
Operator Lifecycle Management
Web Console
Describing OpenShift Extensions
A lot of functionality from Kubernetes depends on external components, such as ingress controllers, storage plug-ins, network plug-ins, and authentication plug-ins. Similar to Linux distributions, there are many ways to build a Kubernetes distribution by picking and choosing different components.
A lot of functionality from Kubernetes also depends on extension APIs, such as access control and network isolation.
OpenShift is a Kubernetes distribution that provides many of these components already integrated and configured, and managed by operators. OpenShift also provides preinstalled applications, such as a container image registry and a web console, managed by operators.

OpenShift brings its own web console with special features for both administrators and developers. OpenShift makes its features available through its own API endpoints which follow the same standard as other Kubernetes APIs using Custom Resource Definitions. OpenShift also brings a full lifecycle management solution that’s deeply integrated which allows for seamless and automatic cluster upgrades from within the cluster itself.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
36
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Infrastructure
services
etcd
NETWORK
COMPUTE
internal and support infrastructure services
Monitoring | Logging | Tuned | SDN | DNS | Kubelet
OpenShift includes a number of internal and support infrastructure services that make containers easier to use at scale.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
37
WORKER / NODE
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
WORKER / NODE
Monitoring | Logging | Tuned SDN | DNS | Kubelet
run on all hosts
And, because these services all run in pods as part of the platform, they can be orchestrated like any other workload and made to run across all hosts in the environment.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
38
INFRA WORKER
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
INFRA WORKER
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Registry
integrated image registry
OpenShift includes an integrated container image registry. This registry is tied into OpenShift’s own role-based access control, providing multi-tenancy benefits. Images that are built inside the platform using OpenShift’s native build features automatically land in this image registry. Being a fully OCI-compliant registry, images created outside of the environment can also be placed into the integrated registry as part of a larger software devleopment lifecycle (SDLC) process.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
39
INFRA WORKER
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
Prometheus | Grafana Alertmanager
INFRA WORKER
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Registry
Prometheus | Grafana Alertmanager
cluster monitoring
OpenShift includes a cluster monitoring solution informed by the best practices of Red Hat’s own site reliability engineers (SREs). This preconfigured stack, based on Prometheus, Grafana, and Alertmanager helps platform administrators understand the health and capacity of the OpenShift cluster.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
40
INFRA WORKER
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
INFRA WORKER
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
log aggregation
OpenShift also includes a log aggregation solution based on Fluentd, Elasticsearch, and Kibana. This integrated solution makes it easy to visualize and corroborate log events for applications that are scaled to many instances, and, it, too, is tied into OpenShift’s role-based access control (RBAC) ensuring that only the right people see the logs they are supposed to.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
41
INFRA WORKER
MASTER (NODE) / CONTROL PLANE
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
Router
INFRA WORKER
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
Router
integrated routing
OpenShift extends Kubernetes ingress capabilities with an integrated router that bridges traffic from outside the cluster into the software defined network (SDN). This routing solution makes it easy for workload to be exposed and made accessible to consumers that are not inside the cluster.

OPENSHIFT CONTAINER PLATFORM | Architectural Overview
42
EXISTING AUTOMATION TOOLSETS
SCM
(GIT)
CI/CD
INFRA WORKER (S)
MASTER
OpenShift Services
STORAGE
Kubernetes
services
Monitoring | Logging | Tuned SDN | DNS | Kubelet
Infrastructure
services
etcd
NETWORK
COMPUTE
Registry
Prometheus | Grafana Alertmanager
Kibana | Elasticsearch
Router
Developers
Admins
WORKER NODES
Monitoring | Logging | Tuned SDN | DNS | Kubelet
dev and ops via web, cli, API, and IDE
Regardless of whether a consumer is an administrator or a developer, a rich set of web, CLI, and IDE integrations, all made possible by the Kubernetes and OpenShift APIs, means that people can work in the way they are most comfortable.

Quiz: Describing the Architecture of OpenShift


Networking
A pluggable model for network interface controls in kubernetes

OpenShift Networking Plug-ins


Product Manager: Marc Curry
3rd-party Kubernetes CNI plug-in certification primarily consists of:
Formalizing the partnership
Certifying the container(s)
Certifying the Operator
Successfully passing the same Kubernetes networking conformance tests that OpenShift uses to validate its own SDN
OPENSHIFT
KUBERNETES CNI
Tech Preview
Cert In-Progress
TBD
Cisco ACI
4.4
VMware NCP
4.4
Juniper Contrail & Tungsten Fabric
Q1CY2021
OpenShift SDN

DEFAULT
4.x
OVN
4.6
kuryr-
kubernetes2
RH-OSP
Neutron
Plugin
4.2.2
Tigera Calico (open src)
4.2+
Fully Supported
45
Version 2021-02-10
VMware Antrea
Q1CY2021
The list of Red Hat GA’d CNI plugins is here:  https://access.redhat.com/articles/4763741
The public-facing list of third-party certified CNI plugins is here: https://access.redhat.com/articles/5436171


OpenShift SDN
An Open Virtual Network OVN Software Defined Network for kubernetes
OpenShift implements a software-defined network (SDN) to manage the network infrastructure of the cluster and user applications. Software-defined networking is a networking model that allows you to manage network services through the abstraction of several networking layers. It decouples the software that handles the traffic, called the control plane, and the underlying mechanisms that route the traffic, called the data plane. Among the many features of SDN, open standards enable vendors to propose their solutions, centralized management, dynamic routing, and tenant isolation.

OpenShift Architecture
47
NODE172.16.1.10
POD10.1.2.2
POD10.1.2.4
NODE172.16.1.20
POD10.1.4.2
POD10.1.4.4
IP Network
VxLAN Overlay Network
OpenShift SDN high-level architecture
Pods are connected via an overlay network which enables IP-per-Pod networking and direct pod-to-pod communication
The Pod IPs are independent of the physical network the OpenShift Nodes are connected to


Using Services for Accessing Pods


routes 
and
ingress
How traffic enters the cluster

50
Routing and Load Balancing


OPENSHIFT NETWORKING | Routes and Ingress
Pluggable routing architecture
HAProxy Router
F5 Router
Multiple-routers with traffic sharding
Router supported protocols
HTTP/HTTPS
WebSockets
TLS with SNI
Non-standard ports via cloud load-balancers, external IP, and NodePort


Exposing Applications for External Access

Guided Exercise: Exposing Applications for External Access


52
Routes vs Ingress


OPENSHIFT NETWORKING | Routes and Ingress
Feature
Ingress
Route
Standard Kubernetes object
X
External access to services
X
X
Persistent (sticky) sessions
X
X
Load-balancing strategies (e.g. round robin)
X
X
Rate-limit and throttling
X
X
IP whitelisting
X
X
TLS edge termination
X
X
TLS re-encryption
X
X
TLS passthrough
X
X
Multiple weighted backends(split traffic)
X
Generated pattern-based hostnames
X
Wildcard domains
X
As of OCP 4.6, Ingress now supports TLS re-encryption and passthrough. Ingress objects are automatically translated into routes. Leaving kubernetes workflows undisturbed

OPENSHIFT NETWORKING | Routes and Ingress
53
Router-based deployment methodologies
SERVICE A
App A
App A
SERVICE B
App B
App B
ROUTE
10% traffic
90% traffic
Split Traffic Between Multiple Services For A/B Testing, Blue/Green and Canary Deployments

Alternative methods for ingress
Different ways that traffic can enter the cluster without the router

OpenShift Architecture
55
Entering the cluster on a random port with service nodeports
NodePort binds a service to a unique port on all the nodes
Traffic received on any node redirects to a node with the running service
Ports in 30K-60K range which usually differs from the service
Firewall rules must allow traffic to all nodes on the specific port
NODE192.10.0.12
NODE192.10.0.11
NODE192.10.0.10
SERVICE
INT IP: 172.1.0.20:90
POD10.1.0.1:90
POD

10.1.0.2:90
POD

10.1.0.3:90
connect 192.10.0.10:31421
192.10.0.11:31421
192.10.0.12:31421
CLIENT
NodePort allow exposing non-standard ports for a service on all nodes in the cluster
If the node receiving the traffic doesn't have the target service running, the traffic is redirected to a node which has the pod running
While it is easier to configure than external IPs and Ingress (prev slide), it is a wasteful use of scarce port resources and it requires more privileges.


OpenShift Architecture
56
NODE192.10.0.12
NODE192.10.0.11
NODE192.10.0.10
External traffic to a service on any port 
with external IP
SERVICE
EXT IP: 200.1.0.10:90
INT IP: 172.1.0.20:90
POD10.1.0.1:90
POD

10.1.0.2:90
POD

10.1.0.3:90
connect 200.1.0.10:90
CLIENT
Access a service with an external IP on any TCP/UDP port, such as
Databases
Message Brokers
Automatic IP allocation from a predefined pool using Ingress IP Self-Service
IP failover pods provide high availability for the IP pool (fully supported in 4.8)

External IP allows assigning an external IP to a service so that client outside of the OpenShift cluster can connect to services inside the Openshift cluster (e.g. database, JMS broker) on that port. This is particularly useful for services that are running on ports other than 80/433, otherwise router is a more suitable alternative. It is also useful when the client needs to know a specific IP for the service in order to open firewalls or while-list it in their own system.
Each external IP address is guaranteed to be assigned to a maximum of one service. This ensures that each service can simply expose its chosen ports regardless of the ports exposed by other services.
The external IP address itself is not managed by the underlying Kubernetes infrastructure and must be maintained and provided by a cluster administrator. Managing external IPs is not convenient since application users cannot create external IP addresses themselves, and must work with cluster administrators to allocate an address for their usage. There is also no protections in place to restrict the usage of a particular external address configured within the cluster. 
Ingress IP self-service is a functionality within OpenShift to streamline the allocation of External IP’s for accessing to services in the cluster. Cluster administrators can designate a range of addresses using a CIDR notation which allows an application user to make a request against the cluster for an external IP address. When a service is configured with the type LoadBalancer, an External IP address will be automatically assigned from the designated range.
Ingress is supported for non-cloud clusters. For cloud (GCE, AWS, and OpenStack) clusters, cloud provider’s load balancer service can be used to automatically deploy a load balancer to target the service’s endpoints.
The IP failover pods are responsible for providing the external IP range (Virtual IPs) and would run on one or more of the nodes (more than one for high availability) in the cluster or they can just run on the HAProxy (router) nodes.
The OpenShift cluster administrator must ensure that the external IP address pool terminates at one or more nodes in the cluster.



OpenShift Architecture
57
OpenShift SDN “flavors”
OPEN NETWORK  (Default)
All pods can communicate with each other across projects
MULTI-TENANT NETWORK
Project-level network isolation
Multicast support
Egress network policies

NODE
POD
POD
POD
POD
NODE
POD
POD
POD
POD
PROJECT A
PROJECT B
DEFAULT NAMESPACE
✓
PROJECT C
Multi-Tenant Network
OpenShift 4 clusters are using OpenShift SDN NetworkPolicy as the default networking solution. The default behavior is wide-open network across projects/namespaces (no policies applied). It is possible to install the cluster in a multitenant mode as an install-time configuration option. In this scenario, all projects/namespaces are network isolated from each other by default, but all can be accessed by the router and can access the cluster registry and other internal services. When installed in this mode, the SDN operator configures the cluster’s project request template to ensure that all new projects/namespaces have isolating policy objects.

Managing Network Policies in OpenShift

Network policies allow you to configure isolation policies for individual pods. Network policies do not require administrative privileges, giving developers more control over the applications in their projects. You can use network policies to create logical zones in the SDN that map to your organization network zones. The benefit of this approach is that the location of running pods becomes irrelevant because network policies allow you to segregate traffic regardless of where it originates.

OpenShift Architecture
59
NetworkPolicy
PROJECT A

POD
POD
POD
POD
PROJECT B

POD
POD
POD
POD
Example Policies
Allow all traffic inside the project
Allow traffic from green to gray
Allow traffic to purple on 8080
✓
✓
8080
5432
✓
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: allow-to-purple-on-8080
spec:
  podSelector:
    matchLabels:
      color: purple
  ingress:
  - ports:
    - protocol: tcp
      port: 8080

✓
NetworkPolicy allows project administrators to configure granular isolation rules using NetworkPolicy objects.
A network policy is a specification of how groups of pods (using labels) are allowed to communicate with each other and other network endpoints
For further details on network policies, refer to this presentation: https://schd.ws/hosted_files/kccncna17/46/ahmetb%20KubeCon%202017%20NA%20%E2%80%93%20Network%20Policies.pdf



OpenShift Architecture
60
OpenShift SDN packet flows
container-container on same host
  NODE
POD 1
veth010.1.15.2/24
br010.1.15.1/24
192.168.0.100
eth0
POD 2
veth110.1.15.3/24
vxlan0

OpenShift Architecture
61
OpenShift SDN packet flows
container-container across hosts
  NODE 2
  NODE 1
POD 1
veth010.1.15.2/24
br010.1.15.1/24
vxlan0
POD 2
veth010.1.20.2/24
br010.1.20.1/24
vxlan0
192.168.0.100
eth0
192.168.0.200
eth0

OpenShift Architecture
62
OpenShift SDN packet flows
container leaving the host
Container to Container on Different Hosts
  NODE 1
POD 1
veth010.1.15.2/24
br010.1.15.1/24
tun0
192.168.0.100
ExternalHost
eth0

Cluster DNS
An automated system for providing hostname resolution within kubernetes

OpenShift Architecture
64
Built-in internal DNS to reach services by a (fully qualified) hostname
Split DNS is used with CoreDNS
CoreDNS answers DNS queries for internal/cluster services
Other defined “upstream” name servers serve the rest of the queries
CoreDNS

Part 1 
(16:15 - 17:00)
Part 2 
(17:05 - 18:00)
Part 3
Day 2 (16:05 - 17:00)
OpenShift Architecture

END of Day 1
